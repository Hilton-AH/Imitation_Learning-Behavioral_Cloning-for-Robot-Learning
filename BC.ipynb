{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYvcjjtZtFYg",
        "outputId": "80c7ae48-7575-4f5c-fda8-c123f3c50868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.13.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, box2d-py, pygame\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.1.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.8/171.8 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install gym[box2d]\n",
        "!pip3 install -q stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zfSwZBrW5EZh"
      },
      "outputs": [],
      "source": [
        "# Part 1 imports\n",
        "from typing import Type, List\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Part 2 imports\n",
        "from torch import optim\n",
        "\n",
        "# Part 3 imports\n",
        "import gym\n",
        "from stable_baselines3.ppo import PPO\n",
        "import torch.nn as nn\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6avALbyl_lCK"
      },
      "source": [
        "Please run the following cells to get the expert data onto your noteboook. Then run `!ls` and verify that the file \"lunarlander_expert.zip\" exits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwsnw2kL_zCB",
        "outputId": "a9635f96-c40a-4fa0-9096-2ff69640f463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-23 03:44:59--  https://github.com/portal-cornell/cs4756_robot_learning/blob/main/assignments/HW1/LunarLander-v2/lunarlander_expert.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/portal-cornell/cs4756_robot_learning/raw/main/assignments/HW1/LunarLander-v2/lunarlander_expert.zip [following]\n",
            "--2023-02-23 03:44:59--  https://github.com/portal-cornell/cs4756_robot_learning/raw/main/assignments/HW1/LunarLander-v2/lunarlander_expert.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/portal-cornell/cs4756_robot_learning/main/assignments/HW1/LunarLander-v2/lunarlander_expert.zip [following]\n",
            "--2023-02-23 03:44:59--  https://raw.githubusercontent.com/portal-cornell/cs4756_robot_learning/main/assignments/HW1/LunarLander-v2/lunarlander_expert.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84995 (83K) [application/zip]\n",
            "Saving to: ‘lunarlander_expert.zip?raw=true’\n",
            "\n",
            "\r          lunarland   0%[                    ]       0  --.-KB/s               \rlunarlander_expert. 100%[===================>]  83.00K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-02-23 03:44:59 (42.7 MB/s) - ‘lunarlander_expert.zip?raw=true’ saved [84995/84995]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://github.com/portal-cornell/cs4756_robot_learning/blob/main/assignments/HW1/LunarLander-v2/lunarlander_expert.zip?raw=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZR7D0YTAADN",
        "outputId": "c1fe4191-dde9-4777-80a7-ab8758b50c22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lunarlander_expert.zip\n"
          ]
        }
      ],
      "source": [
        "!mv lunarlander_expert.zip?raw=true lunarlander_expert.zip\n",
        "!ls | grep \"lunar\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utIlHYCvtf-2"
      },
      "source": [
        "To verify that everything has been downloaded correctly, please run the following cell and check for errors. If none appear, you're good to go! (You may ignore the warnings.warn() messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE45I_o8tfhd",
        "outputId": "fabd6e86-ab97-475e-84c8-11de79371d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: an integer is required (got type bytes)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: an integer is required (got type bytes)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "if PPO.load(\"./lunarlander_expert\"):\n",
        "    print(\"Success!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_OmAcEK9ZVl"
      },
      "source": [
        "**Please make sure there are no errors in the above import statements before continuing to the rest of the assignment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqGreDzW3UtL"
      },
      "source": [
        "### Part 1: Simple utilities for later use\n",
        "\n",
        "In the following section, we will define most of the helper methods that will become useful to you in training and evaluating you imitation agents. There are three sets of utility functions: \"NEURAL NET UTILS\", \"ENV UTILS\", and \"EVAL UTILS.\" Please take the time to understand what each function is doing, and also implement the `argmax_policy()` function in the second cell below. \n",
        "\n",
        "**Note: you may ignore all functions that deal with truncate until you get to the extra credit.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xE0IeZIn4A-B"
      },
      "outputs": [],
      "source": [
        "# ====== NEURAL NET UTILS ======\n",
        "\n",
        "def create_mlp(input_dim: int, output_dim: int, architecture: List[int], squash=False, activation: Type[nn.Module]=nn.ReLU) -> List[nn.Module]:\n",
        "    '''Creates a list of modules that define an MLP.'''\n",
        "    if len(architecture) > 0:\n",
        "        layers = [nn.Linear(input_dim, architecture[0]), activation()]\n",
        "    else:\n",
        "        layers = []\n",
        "        \n",
        "    for i in range(len(architecture) - 1):\n",
        "        layers.append(nn.Linear(architecture[i], architecture[i+1]))\n",
        "        layers.append(activation())\n",
        "    \n",
        "    if output_dim > 0:\n",
        "        last_dim = architecture[-1] if len(architecture) > 0 else input_dim\n",
        "        layers.append(nn.Linear(last_dim, output_dim))\n",
        "        \n",
        "    if squash:\n",
        "        # squashes output down to (-1, 1)\n",
        "        layers.append(nn.Tanh())\n",
        "    \n",
        "    return layers\n",
        "\n",
        "def create_net(input_dim: int, output_dim: int, squash=False):\n",
        "    layers = create_mlp(input_dim, output_dim, architecture=[64, 64], squash=squash)\n",
        "    net = nn.Sequential(*layers)\n",
        "    return net\n",
        "\n",
        "def argmax_policy(net):\n",
        "    # TODO: Return a FUNCTION that takes in a state, passes it through the network, and outputs index of the action with the highest probability.\n",
        "    # Inputs:\n",
        "    # - net: (type nn.Module). A neural network module, going from state dimension to number of actions.\n",
        "    # Wanted output:\n",
        "    # - argmax_fn: A function which takes in a state, and outputs argmax of the action vector.\n",
        "    # return torch.argmax(net)\n",
        "\n",
        "    def argmax_fn(state):\n",
        "        state = torch.from_numpy(state).float() # convert state to tensor and float type so it can be passed through net\n",
        "        values = net(state) # pass through net to get values for each action in state\n",
        "        max_value_index = torch.argmax(values) # get the maximum value from the values for each action in state\n",
        "        return max_value_index\n",
        "        \n",
        "    return argmax_fn\n",
        "\n",
        "def expert_policy(expert, s):\n",
        "    '''Returns a one-hot encoded action of what the expert predicts at state s.'''\n",
        "    action = expert.predict(s)[0]\n",
        "    one_hot_action = np.eye(4)[action]\n",
        "    return one_hot_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "67LY5k2a4Hl3"
      },
      "outputs": [],
      "source": [
        "# ====== ENV UTILS ======\n",
        "\n",
        "def rollout(net, env, truncate=True):\n",
        "    '''Rolls out a trajectory in the environment, with optional state masking.'''\n",
        "    states = []\n",
        "    actions = []\n",
        "    \n",
        "    ob = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    \n",
        "    while not done:\n",
        "        states.append(ob.reshape(-1))\n",
        "        ob_tensor = torch.from_numpy(np.array(ob))\n",
        "        if truncate:\n",
        "            action = net(ob_tensor[:-3].float())\n",
        "        else:\n",
        "            action = net(ob_tensor.float())\n",
        "            \n",
        "        # detach action and convert to np array\n",
        "        if isinstance(action, torch.FloatTensor) or isinstance(action, torch.Tensor):\n",
        "            action = action.detach().numpy()\n",
        "        actions.append(action.reshape(-1))\n",
        "        \n",
        "        # step env\n",
        "        ob, r, done, _ = env.step(np.argmax(action))\n",
        "        total_reward += r\n",
        "        \n",
        "    states = np.array(states, dtype='float')\n",
        "    actions = np.array(actions, dtype='float')\n",
        "    return states, actions\n",
        "\n",
        "def expert_rollout(expert, env, truncate=False):\n",
        "    '''Rolls out an expert trajectory in the environment, with optional state masking.'''\n",
        "    expert_net = lambda s: expert_policy(expert, s)\n",
        "    return rollout(expert_net, env, truncate=truncate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "opfTF8cn4Kkb"
      },
      "outputs": [],
      "source": [
        "# ====== EVAL UTILS ======\n",
        "\n",
        "def eval_policy(policy, env, truncate=True):\n",
        "    '''Evaluates policy with one trajectory in environment. Returns accumulated reward.'''\n",
        "    done = False\n",
        "    ob = env.reset()\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        if truncate:\n",
        "            action = policy(ob[:-2])\n",
        "        else:\n",
        "            action = policy(ob)\n",
        "        \n",
        "        # detach action and convert to np array\n",
        "        if isinstance(action, torch.FloatTensor) or isinstance(action, torch.Tensor):\n",
        "            action = action.detach().numpy()\n",
        "        \n",
        "        # step env and observe reward\n",
        "        ob, r, done, _ = env.step(action)\n",
        "        total_reward += r\n",
        "    \n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4jfPj8d4b7j"
      },
      "source": [
        "### Part 2: Behavioral Cloning & DAgger\n",
        "\n",
        "It is now time to build up our agents! Please read the directions carefully, and avail yourself to the myriad of resources in this class if you feel stuck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGj60deu47oV"
      },
      "source": [
        "**Behavioral cloning:** Behavioral cloning is the simplest imitation learning algorithm, where we perform supervised learning on the given (offline) expert dataset. We either do this via log likelihood maximization (cross entropy minimization) in the discrete action case, or mean-squared error minimization (can also do MLE) in the continuous control setting.\n",
        "\n",
        "Please implement the following `learn()` function for BC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "35nh46Hh4Mdd"
      },
      "outputs": [],
      "source": [
        "class BC:\n",
        "    def __init__(self, net, loss_fn):\n",
        "        self.net = net\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        self.states_history = [] #past history for states\n",
        "        self.actions_history = [] #past history for actions\n",
        "        \n",
        "        self.opt = optim.Adam(self.net.parameters(), lr=3e-4)\n",
        "        \n",
        "    def learn(self, env, states, actions, val_states, val_actions, val_steps=10000, n_steps=1e4, truncate=True):\n",
        "        # TODO: Implement this method. Return the final greedy policy (argmax_policy).\n",
        "\n",
        "        states = torch.from_numpy(states).float() # convert states to tensor and float type\n",
        "        actions = torch.from_numpy(actions).float() # convert actions to tensor and float type\n",
        "\n",
        "        val_states = torch.from_numpy(val_states).float() # convert validation states to tensor and float type\n",
        "        val_actions = torch.from_numpy(val_actions).float() # convert validation actions to tensor and float type\n",
        "\n",
        "\n",
        "        loss_list = [] # list of loss values\n",
        "        reward_list = [] # list of reward values    \n",
        "        val_loss_list = [] # list of validation loss values\n",
        "        for i in range(1, n_steps+1): # loop through n_steps\n",
        "            self.opt.zero_grad() # zero out the gradients\n",
        "            outputs = self.net(states) # pass through net to get values for each action in state\n",
        "            loss = self.loss_fn(outputs, actions) # calculate loss\n",
        "            loss.backward() # backpropagate\n",
        "            self.opt.step() # update weights\n",
        "            loss_list.append(loss.item()) # append loss to loss list\n",
        "            # self.states_history.append(state) # append to the current states\n",
        "            # self.actions_history.append(actions[j]) # append to the current actions\n",
        "\n",
        "            if i % val_steps == 0: # if i is divisible by val_steps\n",
        "                with torch.no_grad(): # turn off gradient tracking\n",
        "                    val_outputs = self.net(val_states) # pass through net to get values for each action in state\n",
        "                    val_loss = self.loss_fn(val_outputs, val_actions) # calculate loss\n",
        "                val_loss_list.append(val_loss.item()) # append loss to loss list\n",
        "            \n",
        "            if i % 100 == 0: # if i is divisible by 100\n",
        "                reward_list.append(np.mean(eval_policy(argmax_policy(self.net), env, truncate)))\n",
        "                \n",
        "        return argmax_policy(self.net), loss_list, reward_list, val_loss_list  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhMweCoM5rkz"
      },
      "source": [
        "**Dataset aggregation (DAgger):** DAgger is a fundamentally interactive algorithm, where we are able to query the expert any time we want to get information about how to proceed. This allows for significantly more freedom for the learner, as it can ask the expert anywhere and not be limited by the dataset that it is given to learn from.\n",
        "\n",
        "Like BC, please implement the following `learn()` function for DAgger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CBEGHL875stw"
      },
      "outputs": [],
      "source": [
        "class DAgger:\n",
        "    def __init__(self, net, loss_fn, expert):\n",
        "        self.net = net\n",
        "        self.loss_fn = loss_fn\n",
        "        self.expert = expert\n",
        "\n",
        "        self.states_history = [] #past history for states\n",
        "        self.actions_history = [] #past history for actions\n",
        "        \n",
        "        self.opt = optim.Adam(self.net.parameters(), lr=3e-4)\n",
        "\n",
        "    #additional loop to append data;  1. rollout using policy 2. Each state, run expert policy 3. append to tuple \n",
        "    def learn(self, env, n_steps=1e4, truncate=True):\n",
        "        # TODO: Implement this method. Return the final greedy policy (argmax_policy).\n",
        "        # Make sure you are making the learning process fundamentally expert-interactive.\n",
        "        self.net.train() # set the network to training mode\n",
        "        reward_list = [] # list to store the rewards\n",
        "        loss_list = [] # list to store the loss\n",
        "\n",
        "        for i in range(1, n_steps+1): # loop over the number of steps to train\n",
        "            self.opt.zero_grad() # zero the gradients\n",
        "            expert_states = rollout(self.net, env, truncate)[0] # rollout the policy and get the states\n",
        "\n",
        "            if truncate: # if truncate is true, remove the last three dimensions, which correspond to angular velocity\n",
        "                expert_states = expert_states[:, :-3] # remove the last three dimensions, which correspond to angular velocity\n",
        "\n",
        "            states = torch.from_numpy(expert_states).float() # convert the states to a tensor\n",
        "            expert_actions = self.net(states) # get the actions from the network\n",
        "            actions = np.empty((len(expert_states), expert_actions.size(1)), dtype=np.float32) # create an empty array to store the actions\n",
        "            \n",
        "            for j, state in enumerate(expert_states): # loop over the states\n",
        "                actions[j] = expert_policy(self.expert, state) # get the expert action for the state\n",
        "                # self.states_history.append(state) # append to the current states\n",
        "                # self.actions_history.append(actions[j]) # append to the current actions\n",
        "\n",
        "            loss = self.loss_fn(expert_actions, torch.from_numpy(actions)) # compute the loss\n",
        "            loss.backward() # backpropagate the loss\n",
        "            self.opt.step() # update the network parameters\n",
        "            loss_list.append(loss.item()) # append the loss to the loss list\n",
        "\n",
        "            if i % 100 == 0: # every 100 steps, evaluate the policy and append the reward to the reward list\n",
        "                reward_list.append(np.mean(eval_policy(argmax_policy(self.net), env, truncate))) \n",
        "        return argmax_policy(self.net), reward_list, loss_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_eQbXOp54dO"
      },
      "source": [
        "### Part 3: Training loop\n",
        "\n",
        "Now with the hard part out of the way, it's time to see the performance of your networks! For imitation learning to work, all you need is access to some expert trajectories. The good news is, we've got you covered! 🙂\n",
        "\n",
        "Please implement the training loop under train() according to the instructions in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QWjgmSRp7kTw"
      },
      "outputs": [],
      "source": [
        "def make_env():\n",
        "    return gym.make(\"LunarLander-v2\")\n",
        "\n",
        "def get_expert():\n",
        "    return PPO.load(\"./lunarlander_expert.zip\")\n",
        "\n",
        "def get_expert_performance(env, expert):\n",
        "  Js = []\n",
        "  for _ in range(100):\n",
        "      obs = env.reset()\n",
        "      J = 0\n",
        "      done = False\n",
        "      hs = []\n",
        "      while not done:\n",
        "          action, _ = expert.predict(obs)\n",
        "          obs, reward, done, info = env.step(action)\n",
        "          hs.append(obs[1])\n",
        "          J += reward\n",
        "      Js.append(J)\n",
        "  ll_expert_performance = np.mean(Js)\n",
        "  return ll_expert_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g4IzXF2cAzkA"
      },
      "outputs": [],
      "source": [
        "def train(train_bc=True, truncate=False, n_steps=10000):\n",
        "    env = make_env()\n",
        "    expert = get_expert()\n",
        "    \n",
        "    performance = get_expert_performance(env, expert)\n",
        "    print('=' * 20)\n",
        "    print(f'Expert performance: {performance}')\n",
        "    print('=' * 20)\n",
        "    \n",
        "    # net + loss fn\n",
        "    if truncate:\n",
        "        net = create_net(input_dim=5, output_dim=4)\n",
        "    else:\n",
        "        net = create_net(input_dim=8, output_dim=4)\n",
        "    \n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    \n",
        "    if train_bc:\n",
        "        # TODO: train BC\n",
        "        # Things that need to be done:\n",
        "        # - Roll out the expert for X number of trajectories (a standard amount is 10).\n",
        "        # - Create our BC learner, and train BC on the collected trajectories.\n",
        "        # - It's up to you how you want to structure your data!\n",
        "        # - Evaluate the argmax_policy by printing the total rewards.\n",
        "\n",
        "        # Create our data so that X number of expert trajectories are rolled out to accumulate data\n",
        "        n_trajectories = 10\n",
        "        expert_states, expert_actions = [], []\n",
        "        for i in range(n_trajectories):\n",
        "            states_traj, actions_traj = expert_rollout(expert, env)\n",
        "            expert_states.append(states_traj)\n",
        "            expert_actions.append(actions_traj)\n",
        "        # Concatenate the trajectory data into single arrays\n",
        "        expert_states = np.concatenate(expert_states, axis=0)\n",
        "        expert_actions = np.concatenate(expert_actions, axis=0)\n",
        "\n",
        "        #create our validation data\n",
        "        val_states, val_actions = [], []\n",
        "        for i in range(n_trajectories):\n",
        "            states_traj, actions_traj = expert_rollout(expert, env)\n",
        "            val_states.append(states_traj)\n",
        "            val_actions.append(actions_traj)\n",
        "\n",
        "        # Concatenate the trajectory data into single arrays\n",
        "        val_states = np.concatenate(val_states, axis=0)\n",
        "        val_actions = np.concatenate(val_actions, axis=0)\n",
        "\n",
        "        # Create our BC learner, and train BC on the collected trajectories.\n",
        "        bc_learner = BC(net, loss_fn)\n",
        "        trained_policy, loss_list, reward_list, val_loss_list = bc_learner.learn(env, expert_states, expert_actions, val_states, val_actions, n_steps=n_steps, truncate=truncate)\n",
        "\n",
        "        # Evaluate the argmax_policy by printing the total rewards.\n",
        "        reward = 0\n",
        "        for i in range(10):\n",
        "            reward += eval_policy(trained_policy, env, truncate=truncate)\n",
        "        print(\"BC Performance: %.12f\" % (reward / 10))\n",
        "\n",
        "        # # Plot the reward over time\n",
        "        # plt.plot(reward_list)\n",
        "        # plt.title(\"Reward over time\")\n",
        "        # plt.xlabel(\"Training steps\")\n",
        "        # plt.ylabel(\"Reward\")\n",
        "        # plt.show()\n",
        "\n",
        "        # # Plot the loss over time\n",
        "        # plt.plot(loss_list)\n",
        "        # plt.title(\"Validation Loss over time\")\n",
        "        # plt.xlabel(\"Training steps\")\n",
        "        # plt.ylabel(\"Loss\")\n",
        "        # plt.show()\n",
        "\n",
        "        # # Plot the validation loss\n",
        "        # plt.plot(val_loss_list)\n",
        "        # plt.title(\"Validation Loss\")\n",
        "        # plt.xlabel(\"Training steps\")\n",
        "        # plt.ylabel(\"Loss\")\n",
        "        # plt.show()\n",
        "\n",
        "    else:\n",
        "        # TODO: train DAgger\n",
        "        # Things that need to be done.\n",
        "        # - Create our DAgger learner.\n",
        "        # - Set up the training loop. Make sure it is fundamentally interactive!\n",
        "        # - It's up to you how you want to structure your data!\n",
        "        # - Evaluate the argmax_policy by printing the total rewards.\n",
        "        \n",
        "        # Create our DAgger learner.\n",
        "        dagger_learner = DAgger(net, loss_fn, expert)\n",
        "\n",
        "        # Set up the training loop. Make sure it is fundamentally interactive!\n",
        "        trained_policy, loss_list, reward_list = dagger_learner.learn(env, n_steps, truncate=truncate)\n",
        "\n",
        "        # Evaluate the argmax_policy by printing the total rewards.\n",
        "        reward = 0\n",
        "        for i in range(20):\n",
        "            reward += eval_policy(trained_policy, env, truncate=truncate)\n",
        "        print(\"DAgger Performance: %.12f\" % (reward / 20))\n",
        "\n",
        "        # # Plot the reward over time\n",
        "        # plt.plot(dagger_learner.reward_list)\n",
        "        # plt.title(\"Reward over time\")\n",
        "        # plt.xlabel(\"Training steps\")\n",
        "        # plt.ylabel(\"Reward\")\n",
        "        # plt.show()\n",
        "\n",
        "        # # Plot the loss over time\n",
        "        # plt.plot(dagger_learner.loss_list)\n",
        "        # plt.title(\"Loss over time\")\n",
        "        # plt.xlabel(\"Training steps\")\n",
        "        # plt.ylabel(\"Loss\")\n",
        "        # plt.show()\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSgu22HQGM6b",
        "outputId": "b45334da-6f0e-48df-a10f-b45fb24b93a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: an integer is required (got type bytes)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: an integer is required (got type bytes)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================\n",
            "Expert performance: 286.82501120676386\n",
            "====================\n",
            "BC Performance: 268.228792372148\n"
          ]
        }
      ],
      "source": [
        "train_bc = True\n",
        "truncate = False\n",
        "n_steps = 1000\n",
        "\n",
        "train(train_bc, truncate, n_steps)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
